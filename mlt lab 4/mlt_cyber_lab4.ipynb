{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6H1OBnEnRg5"
      },
      "source": [
        "# Lab 4 (30-01-2024)\n",
        "\n",
        "This lab experiments help you master how to do linear regression and multiple linear regression.\n",
        "\n",
        "We will be using real estate database provided in lab2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8YQTqvOLYMMY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My name is Deepthi and my roll no : 22011103010\n",
            "Computer IP Address is: 192.168.29.104\n"
          ]
        }
      ],
      "source": [
        "Registration_Number = \"22011103010\"\n",
        "Name = \"Deepthi\"\n",
        "\n",
        "# Python Program to Get IP Address\n",
        "import socket\n",
        "hostname = socket.gethostname()\n",
        "IPAddr = socket.gethostbyname(hostname)\n",
        "\n",
        "print(\"My name is \" + Name + \" and my roll no : \" + Registration_Number)\n",
        "print(\"Computer IP Address is: \" + IPAddr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJLIrF7ooubF"
      },
      "source": [
        "## Experiment 1 - Predicting House prise using the area of the house using Linear regression\n",
        "Load real estate dataset\n",
        " Split the dataset into\n",
        "\n",
        "1.  Split the dataset into train (90%) and test (10%) using scikit learn\n",
        "2.  Fill the cost function\n",
        "3.  Fill the liner regression fit function\n",
        "4.  Fill the routine for Gradient descent\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByNm9jc-1qSs"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tc4VEc2v1nY0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"real_estate.csv\")\n",
        "X = df['Landsize']\n",
        "y = df['Price']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4MOPlLH4m-Vz"
      },
      "outputs": [],
      "source": [
        "# Experiment 1 - Linear regression with one variable\n",
        "# Fill following functions\n",
        "def linear_reg_function(X,w,b):\n",
        "  return w * X + b\n",
        "\n",
        "def cost_function(X,w,b,Y):\n",
        " \n",
        "  cf_val = 0.0\n",
        "  predictions = linear_reg_function(X, w, b)\n",
        "  cf_val = np.sum((predictions - Y) ** 2)\n",
        "  return cf_val\n",
        "  # Compute the cf_val = sum across all i : (wX_i + b)^2\n",
        "\n",
        "def gradient_function(X,w,b,Y):\n",
        "  \n",
        "  dw = 0.0\n",
        "  db = 0.0\n",
        "  predictions = linear_reg_function(X, w, b)\n",
        "  dw = 2 * np.sum((predictions - Y) * X)\n",
        "  db = 2 * np.sum(predictions - Y)\n",
        "  return dw, db\n",
        "  # Compute the grad_val_w, grad_val_b for the cost function\n",
        "\n",
        "\n",
        "  return grad_val_w, grad_val_b\n",
        "\n",
        "# the below function is a template you may edit it\n",
        "def gradient_descent_linear_reg_one_variable(X,Y,model_func, gradient, learning_rate=0.01, converge_param=0.001):\n",
        "  w, wt = 100.0 , 0.0\n",
        "  b, bt = 100.0 , 0.0\n",
        "  num_steps = 1\n",
        "  while np.absolute(w-wt)<converge_param and np.absolute(b-bt)<converge_param:\n",
        "    w = wt\n",
        "    b = bt\n",
        "    cs_val = model_func(X,w,b,Y)\n",
        "    dw, db = gradient(X,w,b,Y)\n",
        "    w -= learning_rate*dw\n",
        "    b -= learning_rate*db\n",
        "    num_steps +=1\n",
        "  return w, b, num_steps\n",
        "\n",
        "# Test with different learning rates and start values and see how gradient descent works\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNDTrxNM8lm0"
      },
      "outputs": [],
      "source": [
        "# once you have found out the optimal w,b plot the gradient function for [w-100,w+100] and [b-100,b+100]\n",
        "# the shape of the curve is important in the convergence of the gradient descent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSjBHV8t8c63"
      },
      "source": [
        "### Experminent 2 - Multiple linear regression\n",
        "use more features and modify the code for more than 1 features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DG8wpaJb_sFx"
      },
      "outputs": [],
      "source": [
        "# Experiment 1 - Linear regression with one variable\n",
        "# Fill following functions\n",
        "def multiple_linear_reg_function(X,w,b):\n",
        "  \"\"\"\n",
        "  Model function for the multiple linear regression\n",
        "  X is the feature Matrix \"For example: f features and m samples\"\n",
        "  Y is the target vector \"For example: Price of the house\"\n",
        "  b is the bias\n",
        "  w is the weight vector with length = f-1\n",
        "  \"\"\"\n",
        "\n",
        "  return\n",
        "\n",
        "def cost_function_multiple(X,w,b,Y):\n",
        "  \"\"\"\n",
        "  Cost function for the linear regression\n",
        "  This is the function that will be minimised using gradient descent\n",
        "  X is the feature Matrix \"For example: f features and m samples\"\n",
        "  Y is the target vector \"For example: Price of the house\"\n",
        "  b is the bias\n",
        "  w is the weight vector with length = f-1\n",
        "  \"\"\"\n",
        "  cf_val = 0.0\n",
        "  # Compute the cf_val = sum across all i : (wX_i + b)^2\n",
        "\n",
        "\n",
        "  return cf_val\n",
        "\n",
        "def gradient_function_multiple(X,w,b,Y):\n",
        "  \"\"\"\n",
        "  Gradient function for the linear regression with one variable\n",
        "  This is the function that will be used to get the gradient in gradient descent\n",
        "  X is the feature Matrix \"For example: f features and m samples\"\n",
        "  Y is the target vector \"For example: Price of the house\"\n",
        "  b is the bias\n",
        "  w is the weight vector with length = f-1\n",
        "  \"\"\"\n",
        "  grad_val_w = 0.0\n",
        "  grad_val_b = 0.0\n",
        "  # Compute the grad_val_w, grad_val_b for the cost function\n",
        "\n",
        "\n",
        "  return grad_val_w, grad_val_b\n",
        "\n",
        "# the below function is a template you may edit it\n",
        "def gradient_descent_linear_reg_multi_variable(X,Y,model_func, gradient, learning_rate=0.01, converge_param=0.001):\n",
        "# Initialise w,b\n",
        "  num_steps = 1\n",
        "  while is_converged(W,B,Wt,Bt):\n",
        "\n",
        "    num_steps +=1\n",
        "  return W, B, num_steps\n",
        "\n",
        "# Similar to previous case, Test with different learning rates and start values and see how gradient descent works\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnU4ZOrQAqpA"
      },
      "source": [
        "# Experiment 3 - Muliple regression\n",
        "1. Use the above code with atleast one feature as categorical variable\n",
        "2. Now use different multiple regression for each category of the variable\n",
        "3. Compare the two methods which is better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UYFtzBCAFQK"
      },
      "outputs": [],
      "source": [
        "# Use plots to see what is happening with the gradient on case 1 and case 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0w_EPZiwBC6n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B96rDLXIB-r8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkBKs6sMB_T6"
      },
      "source": [
        "# Experiment 4 (Optional)\n",
        "Instead of using gradient descent there is another technique which uses the covariance matrix and linear algebra techniques.\n",
        "1. Survey through and find out what that method is.\n",
        "2. Find out the reason why the method is not widely used?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOTa8UYcCUl0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
