{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(weight, bias, X, y):\n",
    "    J = 0.0\n",
    "    n = len(y)\n",
    "    for i in range(n):\n",
    "        J += (weight * X[i] + bias - y[i])**2\n",
    "    J /= (2 * n)\n",
    "    return J\n",
    "\n",
    "# Define function to compute gradients for linear regression\n",
    "def get_gradients(weight, bias, X, y):\n",
    "    dJ_dw = 0.0\n",
    "    dJ_db = 0.0\n",
    "    n = len(y)\n",
    "    for i in range(n):\n",
    "        dJ_dw += (weight * X[i] + bias - y[i]) * X[i]\n",
    "        dJ_db += weight * X[i] + bias - y[i]    \n",
    "    dJ_dw /= n\n",
    "    dJ_db /= n\n",
    "    return dJ_dw, dJ_db\n",
    "\n",
    "# Define gradient descent function for linear regression\n",
    "def gradient_descent(X, y, weight=1.0, bias=1.0, learning_rate=0.9, threshold=0.1):\n",
    "    isConverged = False\n",
    "    weight_ = weight\n",
    "    bias_ = bias\n",
    "    iter_count = 0\n",
    "    while not isConverged:\n",
    "        iter_count += 1\n",
    "        dw, db = get_gradients(weight_, bias_, X, y)\n",
    "        weight_ -= learning_rate * dw\n",
    "        bias_ -= learning_rate * db\n",
    "        if abs(learning_rate * dw) < threshold and abs(learning_rate * db) < threshold:\n",
    "            isConverged = True\n",
    "        weight = weight_\n",
    "        bias = bias_\n",
    "    print(\"Converged in\", iter_count, \"iterations...\")\n",
    "    return weight_, bias_"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
